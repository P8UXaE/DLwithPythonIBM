{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "\n",
    "Conv2d takes in 5 parameters:\n",
    "\n",
    "```in_channels``` that tells how many channels does the constructor take in\n",
    "\n",
    "```out_channels``` that tells how many channels does the constructor returns from each input channel\n",
    "\n",
    "```kernel_size``` that defines the size of the kernel that convolves the image\n",
    "\n",
    "```stride``` that defines if the kernel goes 1 by 1 or has jumps\n",
    "\n",
    "```padding``` that adds an outer border on the image in the case you don't want the output to shrink\n",
    "\n",
    "$$M_{new}=\\dfrac{M+2 \\times padding -K}{stride}+1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[[[ 1.,  0., -1.],\n",
       "                        [ 2.,  0., -2.],\n",
       "                        [ 1.,  0., -1.]]]])),\n",
       "             ('bias', tensor([0.]))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the weights, they are randomly initialized, but you can set them up\n",
    "\n",
    "conv.state_dict()['weight'][0][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\n",
    "conv.state_dict()['bias'][0]=0.0\n",
    "conv.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 1., 0., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 0., 1., 0., 0.],\n",
      "          [0., 0., 1., 0., 0.]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x169abf770>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image=torch.zeros(1,1,5,5)\n",
    "image[0,0,:,2]=1\n",
    "print(image)\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(image[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-4.,  0.,  4.],\n",
       "          [-4.,  0.,  4.],\n",
       "          [-4.,  0.,  4.]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=conv(image)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the output is an important parameter. In this lab, you will assume square images. For rectangular images, the same formula can be used in for each dimension independently.  \n",
    "\n",
    "Let M be the size of the input and K be the size of the kernel. The size of the output is given by the following formula:\n",
    "\n",
    "\n",
    "$$M_{new}=M-K+1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 1, kernel_size=(2, 2), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "K=2\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=K)\n",
    "conv1.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "conv1.state_dict()['bias'][0]=0.0\n",
    "conv1.state_dict()\n",
    "print(conv1)\n",
    "\n",
    "M=4\n",
    "image1=torch.ones(1,1,M,M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output size:\n",
    "\n",
    "$$M_{new}=M-K+1$$\n",
    "$$M_{new}=4-2+1$$\n",
    "$$M_{new}=3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1: tensor([[[[4., 4., 4.],\n",
      "          [4., 4., 4.],\n",
      "          [4., 4., 4.]]]], grad_fn=<ConvolutionBackward0>)\n",
      "shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "z1=conv1(image1)\n",
    "print(\"z1:\",z1)\n",
    "print(\"shape:\",z1.shape[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stride\n",
    "\n",
    "The parameter stride changes the number of shifts the kernel moves per iteration. As a result, the output size also changes and is given by the following formula:\n",
    "\n",
    "$$M_{new}=\\dfrac{M-K}{stride}+1$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[[[1., 1.],\n",
       "                        [1., 1.]]]])),\n",
       "             ('bias', tensor([0.]))])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=2)\n",
    "\n",
    "conv3.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "conv3.state_dict()['bias'][0]=0.0\n",
    "conv3.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an image with a size of 4, calculate the output size:\n",
    "\n",
    "$$M_{new}=\\dfrac{M-K}{stride}+1$$\n",
    "$$M_{new}=\\dfrac{4-2}{2}+1$$\n",
    "$$M_{new}=2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z3: tensor([[[[4., 4.],\n",
      "          [4., 4.]]]], grad_fn=<ConvolutionBackward0>)\n",
      "shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "z3=conv3(image1)\n",
    "\n",
    "print(\"z3:\",z3)\n",
    "print(\"shape:\",z3.shape[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "\n",
    "$$M'=M+2 \\times padding$$\n",
    "$$M_{new}=M'-K+1$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "print(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z4: tensor([[[[4.]]]], grad_fn=<ConvolutionBackward0>)\n",
      "z4: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "conv4 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3)\n",
    "conv4.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "conv4.state_dict()['bias'][0]=0.0\n",
    "conv4.state_dict()\n",
    "z4=conv4(image1)\n",
    "print(\"z4:\",z4)\n",
    "print(\"z4:\",z4.shape[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z5: tensor([[[[1., 2.],\n",
      "          [2., 4.]]]], grad_fn=<ConvolutionBackward0>)\n",
      "z5: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "conv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3,padding=1)\n",
    "\n",
    "conv5.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "conv5.state_dict()['bias'][0]=0.0\n",
    "conv5.state_dict()\n",
    "z5=conv5(image1)\n",
    "print(\"z5:\",z5)\n",
    "print(\"z5:\",z4.shape[2:4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
